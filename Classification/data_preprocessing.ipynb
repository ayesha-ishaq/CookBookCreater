{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/chaimaa.abi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/chaimaa.abi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z4-q1-xEjXFn"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/home/chaimaa.abi/Desktop/AI_project/layer1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "ignore_words = [\n",
    "   'easy','recipe',\"really\",\"breakfast\",\"quick\",\"foolproof\",\"the\",\"lunch\",\"simple\",\"comfort\",\"lovers\",\n",
    "    \"friendly\",\"weeknight\",\"time\",\"miracle\",\"best\",\"everything\",\"heaven\",\"baba\",\"boy\",\"dirt\",\"bliss\",\n",
    "    \"wonderful\",\"momma\",\"fresh\",\"version\",\"just\",\"thumbprint\",\"preserved\",\"lover\",\"tips\",\"city\",\"colorful\",\n",
    "    \"your\",\"bill\",\"joy\",\"treat\",\"substitute\", \"impossible\",\"shortcut\",\"company\",\"market\",\"poor\",\"style\",\"new\",\"elegent\",\n",
    "    \"refreshing\",\"dessert\",\"better\",\"killer\",\"game\",\"sunrise\",\"night\",\"fool\",\"fantastic\",\"uncle\",\"basic\",\"almost\",\"original\",\n",
    "    \"another\",\"mrs\",\"fabulous\",\"speedy\",\"mom\",\"moms\",\"mother\",\"granny\",\"grandmas\",\"our\",\"secret\",\"fall\",\"crazy\",\"dirty\",\n",
    "    \"ingredient\",\"fix\",\"copycat\",\"country\",\"birthday\",\"dad\",\"simply\",\"easiest\",\"chef\",\"lazy\",\"grandma\",\"aunt\",\"authentic\",\"mama\",\n",
    "    \"supper\",\"classic\",\"healthy\",\"healthier\",\"heavenly\",\"magic\",\"slow\",\"ww\",\"favorite\",\"great\",\"ii\",\"iii\",\"iv\",\"yummy\",\"tasty\",\n",
    "    \"party\",\"ultimate\",\"ever\",\"halloween\",\"diabetic\",\"very\",\"day\",\"old\",\"awesome\",\"thanksgiving\",\"amazing\",\"famous\",\"morning\",\"my\",\n",
    "    \"fashioned\",\"holiday\",\"oldfashioned\",\"dinner\",\"minute\",\"surprise\",\"food\",\"good\",\"special\",\"key\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_words_and_characters(title):\n",
    "    new=[]\n",
    "    for word in title.split():\n",
    "        if len(word)>1 and word not in new and word not in ignore_words:\n",
    "            new.append(word)\n",
    "    return \" \".join(new)\n",
    "\n",
    "def clean_title(text):   \n",
    "    text = text.lower()\n",
    "    text =  remove_unnecessary_words_and_characters(text)\n",
    "\n",
    "    words = nltk.word_tokenize(text)  # Tokenize the sentence into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    text = \" \".join(lemmatized_words)  # Join the words back into a sentence\n",
    "    \n",
    "    text=re.sub(r'[:\\*+,-/:;<=>@\\\\^_`(){|}~]+',' ',text)\n",
    "    text=text.replace(r'&amp;?',r' and ')\n",
    "    text=text.replace(r'amp', r' and ')\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mTf_HibPjXFw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1045606/3238442816.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_title_df['clean_title'] = small_title_df.title.apply(clean_title)\n"
     ]
    }
   ],
   "source": [
    "df['sub_col'] = df.title.apply(lambda x:len(str(x).split()))\n",
    "small_title_df = df[df[\"sub_col\"]<=4]\n",
    "small_title_df['clean_title'] = small_title_df.title.apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_names = pd.read_json(\"/home/chaimaa.abi/Downloads/layer2.json\")\n",
    "# joined_df = pd.merge(small_title_df, image_names, on='id')\n",
    "# new_train_part = joined_df[joined_df[\"partition\"]==\"train\"]\n",
    "# title_counts = new_train_part['title'].value_counts()\n",
    "\n",
    "# new_train_part['count'] = new_train_part['title'].map(title_counts)\n",
    "# df_sorted = new_train_part.sort_values(by='count', ascending=False)\n",
    "# df_sorted = df_sorted.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = pd.read_json(\"/home/chaimaa.abi/Desktop/AI_project/layer2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(small_title_df, image_names, on='id')\n",
    "# new_train_part = joined_df[joined_df[\"partition\"]==\"train\"]\n",
    "title_counts = joined_df['title'].value_counts()\n",
    "\n",
    "joined_df['count'] = joined_df['title'].map(title_counts)\n",
    "df_sorted = joined_df.sort_values(by='count', ascending=False)\n",
    "df_sorted = df_sorted.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will keep only 1233 food classes \n",
    "df_sorted = df_sorted[df_sorted['count']>7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation jsons \n",
    "train_ann = []\n",
    "for ind in df_sorted.index:\n",
    "    for each in df_sorted['images'][ind]:\n",
    "        temp_dict = {}\n",
    "        temp_dict['caption'] = df_sorted['clean_title'][ind]\n",
    "        img_id = each['id']\n",
    "        img_path = os.path.join(df_sorted['partition'][ind], \"{}/{}/{}/{}/{}\".format(img_id[0], img_id[1], img_id[2], img_id[3], img_id))\n",
    "        temp_dict['image'] = img_path\n",
    "        train_ann.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = '/home/chaimaa.abi/Desktop/AI_project/all_162_21.json'\n",
    "\n",
    "with open(FILE_PATH, 'w') as output_file:\n",
    "\tjson.dump(train_ann, output_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
